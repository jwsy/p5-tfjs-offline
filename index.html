<html>
<head>
  <meta charset="UTF-8" />
  <title>Webcam Image Classification using TFJS MobileNet and p5.js</title>

  <!-- Load TF.js first -->
  <script src="libs/tf/4.18.0/tf.min.js"></script>

  <!-- Local libs -->
  <script src="libs/p5.js/1.9.0/p5.js"></script>
  <!-- ml5 is not needed for TF.js classification; keep commented to avoid version conflicts -->
  <!-- <script src="libs/ml5/1.2.2/ml5.min.js"></script> -->

  <link rel="stylesheet" href="libs/bootstrap/4.3.1/css/bootstrap.min.css" />
</head>

<body>
  <noscript>Please enable JavaScript to use this app offline.</noscript>

  <div id="info-container" style="padding:20px;">
    <h1>p5-tfjs-demo</h1>
    <h3 id="status">Loading Model...</h3>
    <p>Device: <span id="device-emoji"></span></p>
    <p>Camera dimensions: <span id="camera-dimensions"></span></p>
    <div style="height:5em;">
      <p>
        The TFJS MobileNet model labeled this as:<br />
        <span id="result">...</span><br />
        with a confidence of <span id="probability">...</span>.
      </p>
    </div>
  </div>

  <div id="snapshot-container" style="padding-left:20px; padding-bottom:10px;"></div>
  <div id="canvas-container" style="padding-left:20px;"></div>

  <script>
    let video, model, running = false;
    let LABELS_CACHE = null;

    function isMobileDevice() {
      return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
    }

    async function setup() {
      const canvas = createCanvas(640, 600);
      canvas.parent('canvas-container');

      // Camera
      const constraints = isMobileDevice()
        ? { audio: false, video: { facingMode: "environment" } }
        : { audio: false, video: true };

      select('#device-emoji').html(isMobileDevice() ? 'ðŸ“±' : 'ðŸ’»');

      video = createCapture(constraints);
      video.hide();

      // Load local TFJS MobileNet (LayersModel)
      await tf.ready();
      const modelURL = new URL('/assets/models/mobilenet_v1_0.25_224/model.json', location.href).href;
      model = await tf.loadLayersModel(modelURL);

      await waitForVideo();
      await loadLabels();           // optional; harmless if labels.json is missing
      modelReady();

      const button = createButton('Take a snapshot');
      button.parent('snapshot-container');
      button.mousePressed(snapshot);
    }

    function draw() {
      image(video, 20, 0, 260, 180);
    }

    function waitForVideo() {
      return new Promise((resolve) => {
        const el = video.elt;
        const tick = () => {
          if (el.readyState >= 2 && el.videoWidth > 0 && el.videoHeight > 0) resolve();
          else setTimeout(tick, 50);
        };
        el.onloadeddata = tick;
        tick();
      });
    }

    function modelReady() {
      select('#status').html('Model Loaded');
      select('#camera-dimensions').html(`${video.width}x${video.height}`);
      classifyLoop();
    }

    async function classifyLoop() {
      if (running) return;
      running = true;

      // Build input (224x224, normalize to [-1,1])
      const input = tf.tidy(() =>
        tf.browser.fromPixels(video.elt)
          .resizeBilinear([224, 224])
          .toFloat()
          .div(127.5).sub(1)
          .expandDims(0)            // [1,224,224,3]
      );

      // Predict (do NOT wrap awaits in tidy; dispose after data is read)
      const logits = model.predict(input);
      input.dispose();

      const probs = tf.softmax(logits);
      const arr = await probs.data();
      logits.dispose();
      probs.dispose();

      // Top-K
      const top = topK(Array.from(arr), 5);
      // console.log(top)
      const best = top[0];

      select('#result').html(best.label);
      select('#probability').html(nf(best.probability, 0, 4));

      // Optional highlight
      const hits = ['mortarboard','banjo','hotdog','hot dog'];
      const match = hits.find(h => best.label.includes(h));
      select('#result')
        .style('background-color', match ? 'red' : '')
        .style('font-size', match ? '2em' : '1em');

      running = false;
      requestAnimationFrame(classifyLoop);
    }

    function snapshot() {
      image(video, 20, 300, 340, 230);
    }

    // ---- labels helpers ----
    async function loadLabels() {
      if (LABELS_CACHE) return LABELS_CACHE;
      try {
        const res = await fetch('/assets/models/mobilenet_v1_0.25_224/labels.json');
        if (res.ok) LABELS_CACHE = await res.json();   // array of 1000 strings
      } catch (_) {}
      return LABELS_CACHE;
    }

    function topK(probs, k = 5) {
      const labels = LABELS_CACHE;
      const idx = probs.map((p, i) => [p, i])
                       .sort((a, b) => b[0] - a[0])
                       .slice(0, k);
      return idx.map(([p, i]) => ({
        index: i,
        label: labels ? (labels[i] ?? `class ${i}`) : `class ${i}`,
        probability: p
      }));
    }

    // Optional sanity check callable from DevTools: window.sanity()
    window.sanity = async () => {
      await waitForVideo(); await tf.ready();
      const t0 = performance.now();
      const input = tf.tidy(() =>
        tf.browser.fromPixels(video.elt).resizeBilinear([224,224]).toFloat().div(127.5).sub(1).expandDims(0)
      );
      const logits = model.predict(input); input.dispose();
      const probs = tf.softmax(logits);
      const arr = await probs.data(); logits.dispose(); probs.dispose();
      console.log('Top-1:', topK(Array.from(arr), 1)[0], 'in', (performance.now()-t0).toFixed(1), 'ms');
    };
  </script>
</body>
</html>